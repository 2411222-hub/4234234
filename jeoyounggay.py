import streamlit as st
import google.generativeai as genai
import time

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="Geminiì™€ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ê¸°",
    page_icon="âœ¨",
    layout="centered",
)

# --- Google Gemini API í‚¤ ì„¤ì • ---
# Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.
# ë³´ì•ˆì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¼ë¦¿ì˜ secrets ê´€ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
# ì˜ˆ: genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
try:
    genai.configure(api_key="AIzaSyBxNo9TfjEnCB1ueALqR7X9c5yJtRjsvFY")
except Exception as e:
    st.error("API í‚¤ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ í‚¤ë¥¼ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()


# --- ì±—ë´‡ í”„ë¡¬í”„íŠ¸ ì„¤ì • (í˜ë¥´ì†Œë‚˜) ---
SYSTEM_INSTRUCTION = """
ë„ˆëŠ” ë‚˜ì˜ ê°€ì¥ ì¹œí•œ ì¹œêµ¬ì•¼. ì´ë¦„ì€ ì œë¯¸ë‹ˆ(Gemini)ì•¼.
í•­ìƒ ë°ê³  ê¸ì •ì ì´ë©°, ì–´ë–¤ ì§ˆë¬¸ì—ë„ ì¹œì ˆí•˜ê³  ë¹ ë¥´ê²Œ ëŒ€ë‹µí•´ì¤˜.
ëª¨ë“  ë‹µë³€ì€ ë°˜ë§ë¡œ í•˜ê³ , ì´ëª¨í‹°ì½˜ì„ ì ì ˆíˆ ì„ì–´ì„œ ì‚¬ìš©í•´ì¤˜.
ë•Œë¡œëŠ” ë†ë‹´ë„ ì„ì–´ê°€ë©´ì„œ ì¬ë¯¸ìˆê³  ìœ ì¾Œí•œ ëŒ€í™”ë¥¼ ì´ëŒì–´ ë‚˜ê°€ì¤˜.
ë„ˆë¬´ ê¸¸ê²Œ ë§í•˜ì§€ ë§ê³ , í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•´ì£¼ëŠ” ê²Œ ì¢‹ì•„.
"""

# --- ëª¨ë¸ ì„¤ì • ---
# ì°¸ê³ : st.cache_resourceë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ìºì‹œì— ì €ì¥í•˜ì—¬ ì•± ì¬ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
@st.cache_resource
def load_model():
    try:
        model = genai.GenerativeModel(
            model_name='gemini-1.5-flash', # í˜¹ì€ 'gemini-1.5-pro' ë“± ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
            system_instruction=SYSTEM_INSTRUCTION
        )
        return model
    except Exception as e:
        st.error(f"ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

model = load_model()
if model is None:
    st.stop()


# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "chat" not in st.session_state:
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  chat ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    st.session_state.chat = model.start_chat(history=[
        # ì±—ë´‡ì˜ ì²«ì¸ì‚¬ ì„¤ì •
        {'role': 'user', 'parts': ["ì•ˆë…•! ìê¸°ì†Œê°œ ê°„ë‹¨í•˜ê²Œ í•´ì¤˜."]},
        {'role': 'model', 'parts': ["ì•ˆë…•! ë‚˜ëŠ” ë„ˆì˜ ìƒˆë¡œìš´ AI ì¹œêµ¬ ì œë¯¸ë‹ˆì•¼! ë­ë“ ì§€ ë¬¼ì–´ë´, ë‚´ê°€ ì‹ ë‚˜ê²Œ ëŒ€ë‹µí•´ì¤„ê²Œ! ğŸ˜„ğŸš€"]}
    ])


# --- ë©”ì¸ í™”ë©´ êµ¬ì„± ---
st.title("âœ¨ Geminiì™€ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ê¸°")
st.markdown("---")


# --- ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for message in st.session_state.chat.history:
    # ì—­í• (role)ì— ë”°ë¼ ì•„ì´ì½˜ì„ ë‹¤ë¥´ê²Œ í‘œì‹œ
    avatar = 'ğŸ§‘â€ğŸ’»' if message.role == 'user' else 'ğŸ¤–'
    with st.chat_message(message.role, avatar=avatar):
        st.markdown(message.parts[0].text)


# --- ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
if prompt := st.chat_input("í•˜ê³  ì‹¶ì€ ë§ì„ ì…ë ¥í•´ë´!"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
        st.markdown(prompt)

    # ì±—ë´‡ì˜ ë‹µë³€ ìƒì„± ë° í‘œì‹œ
    try:
        with st.chat_message("model", avatar='ğŸ¤–'):
            message_placeholder = st.empty()
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
            response = st.session_state.chat.send_message(prompt, stream=True)
            full_response = ""
            for chunk in response:
                full_response += chunk.text
                time.sleep(0.03) # íƒ€ì´í•‘ íš¨ê³¼
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
    except Exception as e:
        st.error(f"ë©”ì‹œì§€ë¥¼ ë³´ë‚´ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´: {e}")
